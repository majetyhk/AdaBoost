require(rpart) # for decision stump
require(caret)
# Code has already been provided here to preprocess the data and then call the adaboost function
# Implement the functions marked with ### before this line
data("Ionosphere")
Ionosphere <- Ionosphere[,-c(1,2)]
install.packages("mlbench")
require(mlbench)
data("Ionosphere")
Ionosphere <- Ionosphere[,-c(1,2)]
# lets convert the class labels into format we are familiar with in class
# -1 for bad, 1 for good (create a column named 'Label' which will serve as class variable)
Ionosphere$Label[Ionosphere$Class == "good"] = 1
data("Ionosphere")
View(Ionosphere)
Ionosphere <- Ionosphere[,-c(1,2)]
# lets convert the class labels into format we are familiar with in class
# -1 for bad, 1 for good (create a column named 'Label' which will serve as class variable)
Ionosphere$Label[Ionosphere$Class == "good"] = 1
Ionosphere$Label[Ionosphere$Class == "bad"] = -1
ncol(Ionosphere)
# remove unnecessary columns
Ionosphere <- Ionosphere[,-(ncol(Ionosphere)-1)]
# remove unnecessary columns
Ionosphere <- Ionosphere[,-(ncol(Ionosphere)-1)]
# Code has already been provided here to preprocess the data and then call the adaboost function
# Implement the functions marked with ### before this line
data("Ionosphere")
Ionosphere <- Ionosphere[,-c(1,2)]
# lets convert the class labels into format we are familiar with in class
# -1 for bad, 1 for good (create a column named 'Label' which will serve as class variable)
Ionosphere$Label[Ionosphere$Class == "good"] = 1
Ionosphere$Label[Ionosphere$Class == "bad"] = -1
# remove unnecessary columns
Ionosphere <- Ionosphere[,-(ncol(Ionosphere)-1)]
# class variable
cl <- Ionosphere$Label
# train and predict on training data using adaboost
predictions <- myadaboost(Ionosphere, 5, nrow(Ionosphere))
myadaboost <- function(train, k, n_elements){
fit<-rpart(Label ~ ., data = train, method = "class", control = rpart.control(minsplit = n_elements/2))
return(fit)
}
# train and predict on training data using adaboost
predictions <- myadaboost(Ionosphere, 5, nrow(Ionosphere))
# generate confusion matrix
print(table(cl, predictions))
summary(predictions)
plot(predictions)
plot(predictions, uniform=TRUE,
main="Classification Tree for Kyphosis")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
plot(predictions, uniform=TRUE,
main="Classification Tree for Kyphosis")
text(predictions, use.n=TRUE, all=TRUE, cex=.8)
myadaboost <- function(train, k, n_elements){
fit<-rpart(Label ~ ., data = train, method = "class", control = rpart.control(minsplit = n_elements-1))
return(fit)
}
plot(predictions, uniform=TRUE,
main="Classification Tree for Kyphosis")
text(predictions, use.n=TRUE, all=TRUE, cex=.6)
# train and predict on training data using adaboost
predictions <- myadaboost(Ionosphere, 5, nrow(Ionosphere))
plot(predictions, uniform=TRUE,
main="Classification Tree for Kyphosis")
text(predictions, use.n=TRUE, all=TRUE, cex=.6)
myadaboost <- function(train, k, n_elements){
fit<-rpart(Label ~ ., data = train, method = "class", control = rpart.control(minsplit = n_elements))
return(fit)
}
# train and predict on training data using adaboost
predictions <- myadaboost(Ionosphere, 5, nrow(Ionosphere))
plot(predictions, uniform=TRUE,
main="Classification Tree for Kyphosis")
text(predictions, use.n=TRUE, all=TRUE, cex=.6)
# generate confusion matrix
print(table(cl, predictions))
myadaboost <- function(train, k, n_elements){
fit<-rpart(Label ~ ., data = train, method = "class", control = rpart.control(minsplit = n_elements))
predictions<-predict(fit,train)
predictions
return(predictions)
}
# train and predict on training data using adaboost
predictions <- myadaboost(Ionosphere, 5, nrow(Ionosphere))
predictions
predictions[,-1]
predictions$-1
predictions["-1"]
predictions[,"-1"]
ifelse(predictions[,"-1"]>=0.5,-1,1)
myadaboost <- function(train, k, n_elements){
fit<-rpart(Label ~ ., data = train, method = "class", control = rpart.control(minsplit = n_elements))
predictionProbabilities<-predict(fit,train)
finalPredictions<-ifelse(predictionProbabilities[,"-1"]>=0.5,-1,1)
return(finalPredictions)
}
# train and predict on training data using adaboost
predictions <- myadaboost(Ionosphere, 5, nrow(Ionosphere))
# generate confusion matrix
print(table(cl, predictions))
c(1,2,3)*ifelse(c(1,0,1)!=c(1,0,0),1,0)
rep(1/2,3)
sample(c(1,2,3),size = 10,replace = TRUE, prob = c(0.l,0.1,0.8))
sample(c(1,2,3),size = 10,replace = TRUE, prob = c(0.l,0.1,0.8))
sample(c(1,2,3),size = 10,replace = TRUE, prob = c(.l,.1,.8))
sample(c(1,2,3),size = 10,replace = TRUE, prob = c(0.1,0.1,0.8))
sample(c(1,2,3),size = 10,replace = TRUE, prob = c(0.1,0.1,0.8))
sample(c(1,2,3),size = 10,replace = TRUE, prob = c(0.1,0.1,0.8))
sample(c(1,2,3),size = 10,replace = TRUE, prob = c(0.1,0.1,0.8))
sample(c(1,2,3),size = 10,replace = TRUE, prob = c(0.1,0.1,0.8))
sample(c(1,2,3),size = 10,replace = TRUE, prob = c(0.1,0.1,0.8))
for(i in 1:4){ print(i)}
calculate_alpha <- function(epsilon){
alpha <- 0.5*(log((1-epsilon)/epsilon))
return(alpha)
}
calculate_epsilon <- function(weights, y_true, y_pred, n_elements){
epsilon <- sum(weights*ifelse(y_true != y_pred,1,0))/n_elements
return(epsilon)
}
calculate_weights <- function(old_weights, alpha, y_true, y_pred, n_elements){
correspondingAplhas<-ifelse(y_true == y_pred,-alpha,alpha)
new_weights <- old_weights*exp(correspondingAplhas)
new_weights<-new_weights/sum(new_weights)
return(new_weights)
}
myadaboost <- function(train, k, n_elements){
weights<-rep(1/n_elements,n_elements)
classifiers = list()
#iterationStart
finalPredictions<- rep(0,n_elements)
for (iter in 1:k) {
dataSample<-sample(train,size = n_elements,replace = TRUE,prob = weights)
fit<-rpart(Label ~ .,data=dataSample,method="class",control=rpart.control(minsplit=n_elements))
predictionProbabilities<-predict(fit,train)
predictions<- ifelse(predictionProbabilities[,"-1"]>=0.5,-1,1)
finalPredictions<-finalPredictions + (alpha*predictions)
epsilon<-calculate_epsilon(weights,train$Label, predictions, n_elements)
alpha<-calculate_alpha(epsilon)
weights<-calculate_weights(weights,alpha, train$Label, predictions, n_elements)
}
finalPredictions<-ifelse(finalPredictions>0,1,-1)
return(finalPredictions)
}
# train and predict on training data using adaboost
predictions <- myadaboost(Ionosphere, 5, nrow(Ionosphere))
rep(1/2,3)
# train and predict on training data using adaboost
predictions <- myadaboost(Ionosphere, 5, nrow(Ionosphere))
nrow(Ionosphere)
nel<-nrow(Ionosphere)
weights<-rep(1/nel,nel)
weights
length(weights)
tr<-Ionosphere
dataSample<-sample(train,size = n_elements,replace = TRUE,prob = weights)
dataSample<-sample(tr,size = nel,replace = TRUE,prob = weights)
sum(weights)
ds<-tr[sample(nel,size = nel,replace = TRUE,prob = weights),]
myadaboost <- function(train, k, n_elements){
weights<-rep(1/n_elements,n_elements)
finalPredictions<- rep(0,n_elements)
for (iter in 1:k) {
dataSample<-train[sample(n_elements,size = n_elements,replace = TRUE,prob = weights),]
fit<-rpart(Label ~ .,data=dataSample,method="class",control=rpart.control(minsplit=n_elements))
predictionProbabilities<-predict(fit,train)
predictions<- ifelse(predictionProbabilities[,"-1"]>=0.5,-1,1)
finalPredictions<-finalPredictions + (alpha*predictions)
epsilon<-calculate_epsilon(weights,train$Label, predictions, n_elements)
alpha<-calculate_alpha(epsilon)
weights<-calculate_weights(weights,alpha, train$Label, predictions, n_elements)
}
finalPredictions<-ifelse(finalPredictions>0,1,-1)
return(finalPredictions)
}
# train and predict on training data using adaboost
predictions <- myadaboost(Ionosphere, 5, nrow(Ionosphere))
require(rpart) # for decision stump
require(caret)
require(mlbench)
# set seed to ensure reproducibility
set.seed(100)
# calculate the alpha value using epsilon
# params:
# Input:
# epsilon: value from calculate_epsilon (or error, line 7 in algorithm 5.7 from Textbook)
# output: alpha value (single value) (from Line 12 in algorithm 5.7 from Textbook)
###
calculate_alpha <- function(epsilon){
alpha <- 0.5*(log((1-epsilon)/epsilon))
return(alpha)
}
# calculate the epsilon value
# input:
# weights: weights generated at the end of the previous iteration
# y_true: actual labels (ground truth)
# y_pred: predicted labels (from your newly generated decision stump)
# n_elements: number of elements in y_true or y_pred
# output:
# just the epsilon or error value (line 7 in algorithm 5.7 from Textbook)
###
calculate_epsilon <- function(weights, y_true, y_pred, n_elements){
epsilon <- sum(weights*ifelse(y_true != y_pred,1,0))/n_elements
return(epsilon)
}
# Calculate the weights using equation 5.69 from the textbook
# Input:
# old_weights: weights from previous iteration
# alpha: current alpha value from Line 12 in algorithm 5.7 in the textbook
# y_true: actual class labels
# y_pred: predicted class labels
# n_elements: number of values in y_true or y_pred
# Output:
# a vector of size n_elements containing updated weights
###
calculate_weights <- function(old_weights, alpha, y_true, y_pred, n_elements){
correspondingAplhas<-ifelse(y_true == y_pred,-alpha,alpha)
new_weights <- old_weights*exp(correspondingAplhas)
new_weights<-new_weights/sum(new_weights)
return(new_weights)
}
# implement myadaboost - simple adaboost classification
# use the 'rpart' method from 'rpart' package to create a decision stump
# Think about what parameters you need to set in the rpart method so that it generates only a decision stump, not a decision tree
# Input:
# train: training dataset (attributes + class label)
# k: number of iterations of adaboost
# n_elements: number of elements in 'train'
# Output:
# a vector of predicted values for 'train' after all the iterations of adaboost are completed
###
myadaboost <- function(train, k, n_elements){
weights<-rep(1/n_elements,n_elements)
finalPredictions<- rep(0,n_elements)
for (iter in 1:k) {
dataSample<-train[sample(n_elements,size = n_elements,replace = TRUE,prob = weights),]
fit<-rpart(Label ~ .,data=dataSample,method="class",control=rpart.control(minsplit=n_elements))
predictionProbabilities<-predict(fit,train)
predictions<- ifelse(predictionProbabilities[,"-1"]>=0.5,-1,1)
finalPredictions<-finalPredictions + (alpha*predictions)
epsilon<-calculate_epsilon(weights,train$Label, predictions, n_elements)
alpha<-calculate_alpha(epsilon)
weights<-calculate_weights(weights,alpha, train$Label, predictions, n_elements)
}
finalPredictions<-ifelse(finalPredictions>0,1,-1)
return(finalPredictions)
}
# Code has already been provided here to preprocess the data and then call the adaboost function
# Implement the functions marked with ### before this line
data("Ionosphere")
Ionosphere <- Ionosphere[,-c(1,2)]
# lets convert the class labels into format we are familiar with in class
# -1 for bad, 1 for good (create a column named 'Label' which will serve as class variable)
Ionosphere$Label[Ionosphere$Class == "good"] = 1
Ionosphere$Label[Ionosphere$Class == "bad"] = -1
# remove unnecessary columns
Ionosphere <- Ionosphere[,-(ncol(Ionosphere)-1)]
# class variable
cl <- Ionosphere$Label
# train and predict on training data using adaboost
predictions <- myadaboost(Ionosphere, 5, nrow(Ionosphere))
debugSource('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/Adaboost/hmajety_adb.R', echo=TRUE)
debugSource('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/Adaboost/hmajety_adb.R', echo=TRUE)
debugSource('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/Adaboost/hmajety_adb.R', echo=TRUE)
View(myadaboost)
source('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/Adaboost/hmajety_adb.R', echo=TRUE)
debugSource('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/Adaboost/hmajety_adb.R', echo=TRUE)
source('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/Adaboost/hmajety_adb.R', echo=TRUE)
debugSource('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/Adaboost/hmajety_adb.R', echo=TRUE)
debugSource('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/Adaboost/hmajety_adb.R', echo=TRUE)
debugSource('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/Adaboost/hmajety_adb.R', echo=TRUE)
finalPredictions
debugSource('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/Adaboost/hmajety_adb.R', echo=TRUE)
debugSource('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/Adaboost/hmajety_adb.R', echo=TRUE)
debugSource('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/Adaboost/hmajety_adb.R', echo=TRUE)
174+82
256/351
source('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/Adaboost/hmajety_adb.R', echo=TRUE)
source('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/Adaboost/hmajety_adb.R', echo=TRUE)
source('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/Adaboost/hmajety_adb.R', echo=TRUE)
source('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/Adaboost/hmajety_adb.R')
source('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/Adaboost/hmajety_adb.R')
source('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/Adaboost/hmajety_adb.R')
source('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/Adaboost/hmajety_adb.R')
source('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/Adaboost/hmajety_adb.R')
source('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/Adaboost/hmajety_adb.R')
source('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/Adaboost/hmajety_adb.R')
# set seed to ensure reproducibility
set.seed(101)
source('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/Adaboost/hmajety_adb.R')
source('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/Adaboost/hmajety_adb.R')
source('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/Adaboost/hmajety_adb.R')
source('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/Adaboost/hmajety_adb.R')
source('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/Adaboost/hmajety_adb.R')
myadaboost <- function(train, k, n_elements){
weights<-rep(1/n_elements,n_elements)
finalPredictions<- rep(0,n_elements)
for (iter in 1:k) {
dataSample<-train[sort(sample(n_elements,size = n_elements,replace = TRUE,prob = weights)),]
fit<-rpart(Label ~ .,data=dataSample,parms = list(split = "information"),control=rpart.control(maxdepth = 1))
predictionProbabilities<-predict(fit,train)
predictions<- ifelse(predictionProbabilities>=0.5,1,-1)
epsilon<-calculate_epsilon(weights,train$Label, predictions, n_elements)
if(epsilon>0.5){
weights<-rep(1/n_elements,n_elements)
iter<-iter-1
next
}
alpha<-calculate_alpha(epsilon)
weights<-calculate_weights(weights,alpha, train$Label, predictions, n_elements)
finalPredictions<-finalPredictions + (alpha*predictions)
}
finalPredictions<-ifelse(finalPredictions>0,1,-1)
return(finalPredictions)
}
source('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/Adaboost/hmajety_adb.R')
source('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/Adaboost/hmajety_adb.R')
source('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/Adaboost/hmajety_adb.R')
source('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/Adaboost/hmajety_adb.R')
source('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/Adaboost/hmajety_adb.R')
Ionosphere[,names(Ionosphere)!="Label"]
source('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/Adaboost/hmajety_adb.R')
source('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/Adaboost/hmajety_adb.R')
debugSource('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/Adaboost/hmajety_adb.R')
as.integer(as.character(predictionProbabilities))
source('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/Adaboost/hmajety_adb.R')
seq_len(5)
sample(5,3,replace = TRUE, prob(0.1,.1,.1,.1,.1))
sample(5,3,replace = TRUE, prob= c(0.1,.1,.1,.1,.1))
sample(5,3,replace = TRUE, prob= c(0.2,.2,.2,.2,.2))
sample(5,3,replace = TRUE, prob= c(0.2,.2,.2,.2,.2))
sample(5,3,replace = TRUE, prob= c(0.2,.2,.2,.2,.2))
sample(5,3,replace = TRUE, prob= c(0.2,.2,.2,.2,.2))
sample(5,3,replace = TRUE, prob= c(0.2,.2,.2,.2,.2))
sample(5,3,replace = TRUE, prob= c(0.2,.2,.2,.2,.2))
sample(5,3,replace = TRUE, prob= c(0.2,.2,.2,.2,.2))
sample(5,3,replace = TRUE, prob= c(0.2,.2,.2,.2,.2))
sample(5,3,replace = TRUE, prob= c(0.2,.2,.2,.2,.2))
sample(5,3,replace = TRUE, prob= c(0.2,.2,.2,.2,.2))
sample(5,3,replace = TRUE, prob= c(0.2,.2,.2,.2,.2))
sample(seq_len(5),3,replace = TRUE, prob= c(0.2,.2,.2,.2,.2))
sample(seq_len(5),3,replace = TRUE, prob= c(0.2,.2,.2,.2,.2))
sample(seq_len(5),3,replace = TRUE, prob= c(0.2,.2,.2,.2,.2))
sample(seq_len(5),3,replace = TRUE, prob= c(0.2,.2,.2,.2,.2))
sample(seq_len(5),3,replace = TRUE, prob= c(0.2,.2,.2,.2,.2))
sample(seq_len(5),3,replace = TRUE, prob= c(0.2,.2,.2,.2,.2))
sample(seq_len(5),3,replace = TRUE, prob= c(0.2,.2,.2,.2,.2))
sample(seq_len(5),3,replace = TRUE, prob= c(0.2,.2,.2,.2,.2))
sample(seq_len(5),3,replace = TRUE, prob= c(0.2,.2,.2,.2,.2))
source('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/Adaboost/hmajety_adb.R')
source('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/Adaboost/hmajety_adb.R')
?createFolds
createFolds(Ionosphere,4)
f<-\createFolds(Ionosphere,4)
f<-createFolds(Ionosphere,4)
f$1
f[[1]]
f
k<-vector()
k
k[1]<-1
k
k[3]<-3
k
?knn
fold_accuracies <- c(fold_accuracies, sum(diag(tab)) / sum(tab))
source('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/your_unity_id_knn_cv.R')
debugSource('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/your_unity_id_knn_cv.R')
debugSource('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/your_unity_id_knn_cv.R')
# use ith fold for validation
###
validation_set <- train[folds[[i]], ]
debugSource('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/your_unity_id_knn_cv.R')
debugSource('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/your_unity_id_knn_cv.R')
cl
source('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/your_unity_id_knn_cv.R')
source('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/your_unity_id_knn_cv.R')
source('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/your_unity_id_knn_cv.R')
?createFolds
# create k folds
###
folds <- createFolds(cl, numFolds)
source('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/your_unity_id_knn_cv.R')
source('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/Adaboost/hmajety_adb.R')
source('C:/Users/Hari Krishna M/Dropbox/NCSU/ALDA/HW R2/HW45R/your_unity_id_knn_cv.R')
